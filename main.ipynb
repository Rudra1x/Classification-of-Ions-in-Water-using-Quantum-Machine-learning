{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "efc396b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 48000 samples, class counts: [12000 12000 12000 12000]\n",
      "\n",
      "=== Training variant v6_2 (q=6, layers=2) ===\n",
      "\n",
      "Variant v6_2 - Fold 1/3\n",
      " Pretraining encoder (classical)...\n",
      "  Pretrain epoch 1/8 - loss 1.3892 - val_acc 28.47%\n",
      "  Pretrain epoch 1/8 - loss 1.3892 - val_acc 28.47%\n",
      "  Pretrain epoch 2/8 - loss 1.3796 - val_acc 34.84%\n",
      "  Pretrain epoch 2/8 - loss 1.3796 - val_acc 34.84%\n",
      "  Pretrain epoch 3/8 - loss 1.3626 - val_acc 43.09%\n",
      "  Pretrain epoch 3/8 - loss 1.3626 - val_acc 43.09%\n",
      "  Pretrain epoch 4/8 - loss 1.3202 - val_acc 44.16%\n",
      "  Pretrain epoch 4/8 - loss 1.3202 - val_acc 44.16%\n",
      "  Pretrain epoch 5/8 - loss 1.2510 - val_acc 50.86%\n",
      "  Pretrain epoch 5/8 - loss 1.2510 - val_acc 50.86%\n",
      "  Pretrain epoch 6/8 - loss 1.1840 - val_acc 57.79%\n",
      "  Pretrain epoch 6/8 - loss 1.1840 - val_acc 57.79%\n",
      "  Pretrain epoch 7/8 - loss 1.1314 - val_acc 59.08%\n",
      "  Pretrain epoch 7/8 - loss 1.1314 - val_acc 59.08%\n",
      "  Pretrain epoch 8/8 - loss 1.0913 - val_acc 61.68%\n",
      "  Pretrain epoch 8/8 - loss 1.0913 - val_acc 61.68%\n",
      "  Epoch 1/20 - train_loss 0.8735 - val_acc 50.40%\n",
      "   -> new best val acc 50.40%\n",
      "  Epoch 1/20 - train_loss 0.8735 - val_acc 50.40%\n",
      "   -> new best val acc 50.40%\n",
      "  Epoch 2/20 - train_loss 0.7151 - val_acc 51.96%\n",
      "   -> new best val acc 51.96%\n",
      "  Epoch 2/20 - train_loss 0.7151 - val_acc 51.96%\n",
      "   -> new best val acc 51.96%\n",
      "  Epoch 3/20 - train_loss 0.6618 - val_acc 59.01%\n",
      "   -> new best val acc 59.01%\n",
      "  Epoch 3/20 - train_loss 0.6618 - val_acc 59.01%\n",
      "   -> new best val acc 59.01%\n",
      "  Epoch 4/20 - train_loss 0.6404 - val_acc 58.24%\n",
      "  Epoch 4/20 - train_loss 0.6404 - val_acc 58.24%\n",
      "  Epoch 5/20 - train_loss 0.6150 - val_acc 64.27%\n",
      "   -> new best val acc 64.27%\n",
      "  Epoch 5/20 - train_loss 0.6150 - val_acc 64.27%\n",
      "   -> new best val acc 64.27%\n",
      "  Epoch 6/20 - train_loss 0.6017 - val_acc 66.09%\n",
      "   -> new best val acc 66.09%\n",
      "  Epoch 6/20 - train_loss 0.6017 - val_acc 66.09%\n",
      "   -> new best val acc 66.09%\n",
      "  Epoch 7/20 - train_loss 0.5855 - val_acc 64.37%\n",
      "  Epoch 7/20 - train_loss 0.5855 - val_acc 64.37%\n",
      "  Epoch 8/20 - train_loss 0.5756 - val_acc 65.66%\n",
      "  Epoch 8/20 - train_loss 0.5756 - val_acc 65.66%\n",
      "  Epoch 9/20 - train_loss 0.5638 - val_acc 63.71%\n",
      "  Epoch 9/20 - train_loss 0.5638 - val_acc 63.71%\n",
      "  Epoch 10/20 - train_loss 0.5545 - val_acc 65.92%\n",
      "  Epoch 10/20 - train_loss 0.5545 - val_acc 65.92%\n",
      "  Epoch 11/20 - train_loss 0.5408 - val_acc 65.11%\n",
      "  Epoch 11/20 - train_loss 0.5408 - val_acc 65.11%\n",
      "  Epoch 12/20 - train_loss 0.5389 - val_acc 64.58%\n",
      "   -> early stopping\n",
      "  Epoch 12/20 - train_loss 0.5389 - val_acc 64.58%\n",
      "   -> early stopping\n",
      "\n",
      "Variant v6_2 - Fold 2/3\n",
      " Pretraining encoder (classical)...\n",
      "\n",
      "Variant v6_2 - Fold 2/3\n",
      " Pretraining encoder (classical)...\n",
      "  Pretrain epoch 1/8 - loss 1.3868 - val_acc 26.41%\n",
      "  Pretrain epoch 1/8 - loss 1.3868 - val_acc 26.41%\n",
      "  Pretrain epoch 2/8 - loss 1.3807 - val_acc 35.84%\n",
      "  Pretrain epoch 2/8 - loss 1.3807 - val_acc 35.84%\n",
      "  Pretrain epoch 3/8 - loss 1.3664 - val_acc 36.20%\n",
      "  Pretrain epoch 3/8 - loss 1.3664 - val_acc 36.20%\n",
      "  Pretrain epoch 4/8 - loss 1.3277 - val_acc 50.54%\n",
      "  Pretrain epoch 4/8 - loss 1.3277 - val_acc 50.54%\n",
      "  Pretrain epoch 5/8 - loss 1.2616 - val_acc 51.07%\n",
      "  Pretrain epoch 5/8 - loss 1.2616 - val_acc 51.07%\n",
      "  Pretrain epoch 6/8 - loss 1.1932 - val_acc 59.49%\n",
      "  Pretrain epoch 6/8 - loss 1.1932 - val_acc 59.49%\n",
      "  Pretrain epoch 7/8 - loss 1.1403 - val_acc 57.53%\n",
      "  Pretrain epoch 7/8 - loss 1.1403 - val_acc 57.53%\n",
      "  Pretrain epoch 8/8 - loss 1.1006 - val_acc 59.17%\n",
      "  Pretrain epoch 8/8 - loss 1.1006 - val_acc 59.17%\n",
      "  Epoch 1/20 - train_loss 0.9480 - val_acc 47.51%\n",
      "   -> new best val acc 47.51%\n",
      "  Epoch 1/20 - train_loss 0.9480 - val_acc 47.51%\n",
      "   -> new best val acc 47.51%\n",
      "  Epoch 2/20 - train_loss 0.7824 - val_acc 49.11%\n",
      "   -> new best val acc 49.11%\n",
      "  Epoch 2/20 - train_loss 0.7824 - val_acc 49.11%\n",
      "   -> new best val acc 49.11%\n",
      "  Epoch 3/20 - train_loss 0.7081 - val_acc 53.94%\n",
      "   -> new best val acc 53.94%\n",
      "  Epoch 3/20 - train_loss 0.7081 - val_acc 53.94%\n",
      "   -> new best val acc 53.94%\n",
      "  Epoch 4/20 - train_loss 0.6743 - val_acc 57.14%\n",
      "   -> new best val acc 57.14%\n",
      "  Epoch 4/20 - train_loss 0.6743 - val_acc 57.14%\n",
      "   -> new best val acc 57.14%\n",
      "  Epoch 5/20 - train_loss 0.6521 - val_acc 54.73%\n",
      "  Epoch 5/20 - train_loss 0.6521 - val_acc 54.73%\n",
      "  Epoch 6/20 - train_loss 0.6309 - val_acc 56.45%\n",
      "  Epoch 6/20 - train_loss 0.6309 - val_acc 56.45%\n",
      "  Epoch 7/20 - train_loss 0.6110 - val_acc 54.86%\n",
      "  Epoch 7/20 - train_loss 0.6110 - val_acc 54.86%\n",
      "  Epoch 8/20 - train_loss 0.5956 - val_acc 59.54%\n",
      "   -> new best val acc 59.54%\n",
      "  Epoch 8/20 - train_loss 0.5956 - val_acc 59.54%\n",
      "   -> new best val acc 59.54%\n",
      "  Epoch 9/20 - train_loss 0.5795 - val_acc 61.29%\n",
      "   -> new best val acc 61.29%\n",
      "  Epoch 9/20 - train_loss 0.5795 - val_acc 61.29%\n",
      "   -> new best val acc 61.29%\n",
      "  Epoch 10/20 - train_loss 0.5711 - val_acc 60.79%\n",
      "  Epoch 10/20 - train_loss 0.5711 - val_acc 60.79%\n",
      "  Epoch 11/20 - train_loss 0.5560 - val_acc 60.04%\n",
      "  Epoch 11/20 - train_loss 0.5560 - val_acc 60.04%\n",
      "  Epoch 12/20 - train_loss 0.5470 - val_acc 62.16%\n",
      "   -> new best val acc 62.16%\n",
      "  Epoch 12/20 - train_loss 0.5470 - val_acc 62.16%\n",
      "   -> new best val acc 62.16%\n",
      "  Epoch 13/20 - train_loss 0.5390 - val_acc 66.04%\n",
      "   -> new best val acc 66.04%\n",
      "  Epoch 13/20 - train_loss 0.5390 - val_acc 66.04%\n",
      "   -> new best val acc 66.04%\n",
      "  Epoch 14/20 - train_loss 0.5374 - val_acc 58.71%\n",
      "  Epoch 14/20 - train_loss 0.5374 - val_acc 58.71%\n",
      "  Epoch 15/20 - train_loss 0.5300 - val_acc 63.18%\n",
      "  Epoch 15/20 - train_loss 0.5300 - val_acc 63.18%\n",
      "  Epoch 16/20 - train_loss 0.5204 - val_acc 64.27%\n",
      "  Epoch 16/20 - train_loss 0.5204 - val_acc 64.27%\n",
      "  Epoch 17/20 - train_loss 0.5156 - val_acc 60.48%\n",
      "  Epoch 17/20 - train_loss 0.5156 - val_acc 60.48%\n",
      "  Epoch 18/20 - train_loss 0.5068 - val_acc 65.19%\n",
      "  Epoch 18/20 - train_loss 0.5068 - val_acc 65.19%\n",
      "  Epoch 19/20 - train_loss 0.5039 - val_acc 64.37%\n",
      "   -> early stopping\n",
      "  Epoch 19/20 - train_loss 0.5039 - val_acc 64.37%\n",
      "   -> early stopping\n",
      "\n",
      "Variant v6_2 - Fold 3/3\n",
      " Pretraining encoder (classical)...\n",
      "\n",
      "Variant v6_2 - Fold 3/3\n",
      " Pretraining encoder (classical)...\n",
      "  Pretrain epoch 1/8 - loss 1.3879 - val_acc 29.58%\n",
      "  Pretrain epoch 1/8 - loss 1.3879 - val_acc 29.58%\n",
      "  Pretrain epoch 2/8 - loss 1.3771 - val_acc 34.27%\n",
      "  Pretrain epoch 2/8 - loss 1.3771 - val_acc 34.27%\n",
      "  Pretrain epoch 3/8 - loss 1.3564 - val_acc 35.84%\n",
      "  Pretrain epoch 3/8 - loss 1.3564 - val_acc 35.84%\n",
      "  Pretrain epoch 4/8 - loss 1.3145 - val_acc 43.29%\n",
      "  Pretrain epoch 4/8 - loss 1.3145 - val_acc 43.29%\n",
      "  Pretrain epoch 5/8 - loss 1.2596 - val_acc 46.56%\n",
      "  Pretrain epoch 5/8 - loss 1.2596 - val_acc 46.56%\n",
      "  Pretrain epoch 6/8 - loss 1.2046 - val_acc 56.91%\n",
      "  Pretrain epoch 6/8 - loss 1.2046 - val_acc 56.91%\n",
      "  Pretrain epoch 7/8 - loss 1.1614 - val_acc 56.04%\n",
      "  Pretrain epoch 7/8 - loss 1.1614 - val_acc 56.04%\n",
      "  Pretrain epoch 8/8 - loss 1.1312 - val_acc 56.67%\n",
      "  Pretrain epoch 8/8 - loss 1.1312 - val_acc 56.67%\n",
      "  Epoch 1/20 - train_loss 0.9203 - val_acc 42.30%\n",
      "   -> new best val acc 42.30%\n",
      "  Epoch 1/20 - train_loss 0.9203 - val_acc 42.30%\n",
      "   -> new best val acc 42.30%\n",
      "  Epoch 2/20 - train_loss 0.7905 - val_acc 48.39%\n",
      "   -> new best val acc 48.39%\n",
      "  Epoch 2/20 - train_loss 0.7905 - val_acc 48.39%\n",
      "   -> new best val acc 48.39%\n",
      "  Epoch 3/20 - train_loss 0.7317 - val_acc 52.15%\n",
      "   -> new best val acc 52.15%\n",
      "  Epoch 3/20 - train_loss 0.7317 - val_acc 52.15%\n",
      "   -> new best val acc 52.15%\n",
      "  Epoch 4/20 - train_loss 0.6987 - val_acc 55.26%\n",
      "   -> new best val acc 55.26%\n",
      "  Epoch 4/20 - train_loss 0.6987 - val_acc 55.26%\n",
      "   -> new best val acc 55.26%\n",
      "  Epoch 5/20 - train_loss 0.6790 - val_acc 50.98%\n",
      "  Epoch 5/20 - train_loss 0.6790 - val_acc 50.98%\n",
      "  Epoch 6/20 - train_loss 0.6624 - val_acc 56.57%\n",
      "   -> new best val acc 56.57%\n",
      "  Epoch 6/20 - train_loss 0.6624 - val_acc 56.57%\n",
      "   -> new best val acc 56.57%\n",
      "  Epoch 7/20 - train_loss 0.6464 - val_acc 53.34%\n",
      "  Epoch 7/20 - train_loss 0.6464 - val_acc 53.34%\n",
      "  Epoch 8/20 - train_loss 0.6318 - val_acc 54.58%\n",
      "  Epoch 8/20 - train_loss 0.6318 - val_acc 54.58%\n",
      "  Epoch 9/20 - train_loss 0.6244 - val_acc 56.39%\n",
      "  Epoch 9/20 - train_loss 0.6244 - val_acc 56.39%\n",
      "  Epoch 10/20 - train_loss 0.6083 - val_acc 57.26%\n",
      "   -> new best val acc 57.26%\n",
      "  Epoch 10/20 - train_loss 0.6083 - val_acc 57.26%\n",
      "   -> new best val acc 57.26%\n",
      "  Epoch 11/20 - train_loss 0.5966 - val_acc 59.32%\n",
      "   -> new best val acc 59.32%\n",
      "  Epoch 11/20 - train_loss 0.5966 - val_acc 59.32%\n",
      "   -> new best val acc 59.32%\n",
      "  Epoch 12/20 - train_loss 0.5913 - val_acc 58.94%\n",
      "  Epoch 12/20 - train_loss 0.5913 - val_acc 58.94%\n",
      "  Epoch 13/20 - train_loss 0.5767 - val_acc 57.40%\n",
      "  Epoch 13/20 - train_loss 0.5767 - val_acc 57.40%\n",
      "  Epoch 14/20 - train_loss 0.5669 - val_acc 59.81%\n",
      "   -> new best val acc 59.81%\n",
      "  Epoch 14/20 - train_loss 0.5669 - val_acc 59.81%\n",
      "   -> new best val acc 59.81%\n",
      "  Epoch 15/20 - train_loss 0.5606 - val_acc 59.64%\n",
      "  Epoch 15/20 - train_loss 0.5606 - val_acc 59.64%\n",
      "  Epoch 16/20 - train_loss 0.5495 - val_acc 62.02%\n",
      "   -> new best val acc 62.02%\n",
      "  Epoch 16/20 - train_loss 0.5495 - val_acc 62.02%\n",
      "   -> new best val acc 62.02%\n",
      "  Epoch 17/20 - train_loss 0.5456 - val_acc 60.89%\n",
      "  Epoch 17/20 - train_loss 0.5456 - val_acc 60.89%\n",
      "  Epoch 18/20 - train_loss 0.5382 - val_acc 59.98%\n",
      "  Epoch 18/20 - train_loss 0.5382 - val_acc 59.98%\n",
      "  Epoch 19/20 - train_loss 0.5310 - val_acc 62.49%\n",
      "   -> new best val acc 62.49%\n",
      "  Epoch 19/20 - train_loss 0.5310 - val_acc 62.49%\n",
      "   -> new best val acc 62.49%\n",
      "  Epoch 20/20 - train_loss 0.5250 - val_acc 61.28%\n",
      "  Epoch 20/20 - train_loss 0.5250 - val_acc 61.28%\n",
      "Variant v6_2 OOF acc: 64.88%\n",
      "\n",
      "=== Training variant v7_3 (q=7, layers=3) ===\n",
      "\n",
      "Variant v7_3 - Fold 1/3\n",
      " Pretraining encoder (classical)...\n",
      "Variant v6_2 OOF acc: 64.88%\n",
      "\n",
      "=== Training variant v7_3 (q=7, layers=3) ===\n",
      "\n",
      "Variant v7_3 - Fold 1/3\n",
      " Pretraining encoder (classical)...\n",
      "  Pretrain epoch 1/8 - loss 1.3906 - val_acc 24.57%\n",
      "  Pretrain epoch 1/8 - loss 1.3906 - val_acc 24.57%\n",
      "  Pretrain epoch 2/8 - loss 1.3859 - val_acc 24.80%\n",
      "  Pretrain epoch 2/8 - loss 1.3859 - val_acc 24.80%\n",
      "  Pretrain epoch 3/8 - loss 1.3804 - val_acc 41.05%\n",
      "  Pretrain epoch 3/8 - loss 1.3804 - val_acc 41.05%\n",
      "  Pretrain epoch 4/8 - loss 1.3626 - val_acc 45.21%\n",
      "  Pretrain epoch 4/8 - loss 1.3626 - val_acc 45.21%\n",
      "  Pretrain epoch 5/8 - loss 1.3157 - val_acc 49.03%\n",
      "  Pretrain epoch 5/8 - loss 1.3157 - val_acc 49.03%\n",
      "  Pretrain epoch 6/8 - loss 1.2469 - val_acc 57.93%\n",
      "  Pretrain epoch 6/8 - loss 1.2469 - val_acc 57.93%\n",
      "  Pretrain epoch 7/8 - loss 1.1790 - val_acc 60.31%\n",
      "  Pretrain epoch 7/8 - loss 1.1790 - val_acc 60.31%\n",
      "  Pretrain epoch 8/8 - loss 1.1188 - val_acc 60.16%\n",
      "  Pretrain epoch 8/8 - loss 1.1188 - val_acc 60.16%\n",
      "  Epoch 1/20 - train_loss 0.9192 - val_acc 47.68%\n",
      "   -> new best val acc 47.68%\n",
      "  Epoch 1/20 - train_loss 0.9192 - val_acc 47.68%\n",
      "   -> new best val acc 47.68%\n",
      "  Epoch 2/20 - train_loss 0.7534 - val_acc 56.57%\n",
      "   -> new best val acc 56.57%\n",
      "  Epoch 2/20 - train_loss 0.7534 - val_acc 56.57%\n",
      "   -> new best val acc 56.57%\n",
      "  Epoch 3/20 - train_loss 0.6767 - val_acc 56.91%\n",
      "   -> new best val acc 56.91%\n",
      "  Epoch 3/20 - train_loss 0.6767 - val_acc 56.91%\n",
      "   -> new best val acc 56.91%\n",
      "  Epoch 4/20 - train_loss 0.6328 - val_acc 58.26%\n",
      "   -> new best val acc 58.26%\n",
      "  Epoch 4/20 - train_loss 0.6328 - val_acc 58.26%\n",
      "   -> new best val acc 58.26%\n",
      "  Epoch 5/20 - train_loss 0.6103 - val_acc 60.44%\n",
      "   -> new best val acc 60.44%\n",
      "  Epoch 5/20 - train_loss 0.6103 - val_acc 60.44%\n",
      "   -> new best val acc 60.44%\n",
      "  Epoch 6/20 - train_loss 0.5922 - val_acc 61.08%\n",
      "   -> new best val acc 61.08%\n",
      "  Epoch 6/20 - train_loss 0.5922 - val_acc 61.08%\n",
      "   -> new best val acc 61.08%\n",
      "  Epoch 7/20 - train_loss 0.5763 - val_acc 64.36%\n",
      "   -> new best val acc 64.36%\n",
      "  Epoch 7/20 - train_loss 0.5763 - val_acc 64.36%\n",
      "   -> new best val acc 64.36%\n",
      "  Epoch 8/20 - train_loss 0.5604 - val_acc 63.60%\n",
      "  Epoch 8/20 - train_loss 0.5604 - val_acc 63.60%\n",
      "  Epoch 9/20 - train_loss 0.5516 - val_acc 62.44%\n",
      "  Epoch 9/20 - train_loss 0.5516 - val_acc 62.44%\n",
      "  Epoch 10/20 - train_loss 0.5370 - val_acc 62.86%\n",
      "  Epoch 10/20 - train_loss 0.5370 - val_acc 62.86%\n",
      "  Epoch 11/20 - train_loss 0.5252 - val_acc 66.07%\n",
      "   -> new best val acc 66.07%\n",
      "  Epoch 11/20 - train_loss 0.5252 - val_acc 66.07%\n",
      "   -> new best val acc 66.07%\n",
      "  Epoch 12/20 - train_loss 0.5178 - val_acc 65.59%\n",
      "  Epoch 12/20 - train_loss 0.5178 - val_acc 65.59%\n",
      "  Epoch 13/20 - train_loss 0.5090 - val_acc 63.66%\n",
      "  Epoch 13/20 - train_loss 0.5090 - val_acc 63.66%\n",
      "  Epoch 14/20 - train_loss 0.5041 - val_acc 65.29%\n",
      "  Epoch 14/20 - train_loss 0.5041 - val_acc 65.29%\n",
      "  Epoch 15/20 - train_loss 0.4965 - val_acc 67.03%\n",
      "   -> new best val acc 67.03%\n",
      "  Epoch 15/20 - train_loss 0.4965 - val_acc 67.03%\n",
      "   -> new best val acc 67.03%\n",
      "  Epoch 16/20 - train_loss 0.4885 - val_acc 62.98%\n",
      "  Epoch 16/20 - train_loss 0.4885 - val_acc 62.98%\n",
      "  Epoch 17/20 - train_loss 0.4834 - val_acc 66.22%\n",
      "  Epoch 17/20 - train_loss 0.4834 - val_acc 66.22%\n",
      "  Epoch 18/20 - train_loss 0.4791 - val_acc 64.42%\n",
      "  Epoch 18/20 - train_loss 0.4791 - val_acc 64.42%\n",
      "  Epoch 19/20 - train_loss 0.4747 - val_acc 64.86%\n",
      "  Epoch 19/20 - train_loss 0.4747 - val_acc 64.86%\n",
      "  Epoch 20/20 - train_loss 0.4707 - val_acc 65.45%\n",
      "  Epoch 20/20 - train_loss 0.4707 - val_acc 65.45%\n",
      "\n",
      "Variant v7_3 - Fold 2/3\n",
      " Pretraining encoder (classical)...\n",
      "\n",
      "Variant v7_3 - Fold 2/3\n",
      " Pretraining encoder (classical)...\n",
      "  Pretrain epoch 1/8 - loss 1.3887 - val_acc 29.46%\n",
      "  Pretrain epoch 1/8 - loss 1.3887 - val_acc 29.46%\n",
      "  Pretrain epoch 2/8 - loss 1.3845 - val_acc 29.04%\n",
      "  Pretrain epoch 2/8 - loss 1.3845 - val_acc 29.04%\n",
      "  Pretrain epoch 3/8 - loss 1.3778 - val_acc 38.89%\n",
      "  Pretrain epoch 3/8 - loss 1.3778 - val_acc 38.89%\n",
      "  Pretrain epoch 4/8 - loss 1.3555 - val_acc 55.02%\n",
      "  Pretrain epoch 4/8 - loss 1.3555 - val_acc 55.02%\n",
      "  Pretrain epoch 5/8 - loss 1.3072 - val_acc 42.81%\n",
      "  Pretrain epoch 5/8 - loss 1.3072 - val_acc 42.81%\n",
      "  Pretrain epoch 6/8 - loss 1.2358 - val_acc 52.06%\n",
      "  Pretrain epoch 6/8 - loss 1.2358 - val_acc 52.06%\n",
      "  Pretrain epoch 7/8 - loss 1.1647 - val_acc 61.86%\n",
      "  Pretrain epoch 7/8 - loss 1.1647 - val_acc 61.86%\n",
      "  Pretrain epoch 8/8 - loss 1.1108 - val_acc 55.24%\n",
      "  Pretrain epoch 8/8 - loss 1.1108 - val_acc 55.24%\n",
      "  Epoch 1/20 - train_loss 0.9344 - val_acc 45.79%\n",
      "   -> new best val acc 45.79%\n",
      "  Epoch 1/20 - train_loss 0.9344 - val_acc 45.79%\n",
      "   -> new best val acc 45.79%\n",
      "  Epoch 2/20 - train_loss 0.7474 - val_acc 52.09%\n",
      "   -> new best val acc 52.09%\n",
      "  Epoch 2/20 - train_loss 0.7474 - val_acc 52.09%\n",
      "   -> new best val acc 52.09%\n",
      "  Epoch 3/20 - train_loss 0.6519 - val_acc 62.56%\n",
      "   -> new best val acc 62.56%\n",
      "  Epoch 3/20 - train_loss 0.6519 - val_acc 62.56%\n",
      "   -> new best val acc 62.56%\n",
      "  Epoch 4/20 - train_loss 0.6085 - val_acc 56.21%\n",
      "  Epoch 4/20 - train_loss 0.6085 - val_acc 56.21%\n",
      "  Epoch 5/20 - train_loss 0.5792 - val_acc 65.18%\n",
      "   -> new best val acc 65.18%\n",
      "  Epoch 5/20 - train_loss 0.5792 - val_acc 65.18%\n",
      "   -> new best val acc 65.18%\n",
      "  Epoch 6/20 - train_loss 0.5574 - val_acc 58.96%\n",
      "  Epoch 6/20 - train_loss 0.5574 - val_acc 58.96%\n",
      "  Epoch 7/20 - train_loss 0.5368 - val_acc 63.74%\n",
      "  Epoch 7/20 - train_loss 0.5368 - val_acc 63.74%\n",
      "  Epoch 8/20 - train_loss 0.5215 - val_acc 63.74%\n",
      "  Epoch 8/20 - train_loss 0.5215 - val_acc 63.74%\n",
      "  Epoch 9/20 - train_loss 0.5101 - val_acc 61.68%\n",
      "  Epoch 9/20 - train_loss 0.5101 - val_acc 61.68%\n",
      "  Epoch 10/20 - train_loss 0.5027 - val_acc 62.15%\n",
      "  Epoch 10/20 - train_loss 0.5027 - val_acc 62.15%\n",
      "  Epoch 11/20 - train_loss 0.4940 - val_acc 64.16%\n",
      "   -> early stopping\n",
      "  Epoch 11/20 - train_loss 0.4940 - val_acc 64.16%\n",
      "   -> early stopping\n",
      "\n",
      "Variant v7_3 - Fold 3/3\n",
      " Pretraining encoder (classical)...\n",
      "\n",
      "Variant v7_3 - Fold 3/3\n",
      " Pretraining encoder (classical)...\n",
      "  Pretrain epoch 1/8 - loss 1.3881 - val_acc 25.85%\n",
      "  Pretrain epoch 1/8 - loss 1.3881 - val_acc 25.85%\n",
      "  Pretrain epoch 2/8 - loss 1.3816 - val_acc 25.73%\n",
      "  Pretrain epoch 2/8 - loss 1.3816 - val_acc 25.73%\n",
      "  Pretrain epoch 3/8 - loss 1.3706 - val_acc 44.74%\n",
      "  Pretrain epoch 3/8 - loss 1.3706 - val_acc 44.74%\n",
      "  Pretrain epoch 4/8 - loss 1.3444 - val_acc 41.85%\n",
      "  Pretrain epoch 4/8 - loss 1.3444 - val_acc 41.85%\n",
      "  Pretrain epoch 5/8 - loss 1.2844 - val_acc 47.75%\n",
      "  Pretrain epoch 5/8 - loss 1.2844 - val_acc 47.75%\n",
      "  Pretrain epoch 6/8 - loss 1.2078 - val_acc 56.47%\n",
      "  Pretrain epoch 6/8 - loss 1.2078 - val_acc 56.47%\n",
      "  Pretrain epoch 7/8 - loss 1.1479 - val_acc 57.32%\n",
      "  Pretrain epoch 7/8 - loss 1.1479 - val_acc 57.32%\n",
      "  Pretrain epoch 8/8 - loss 1.0990 - val_acc 61.91%\n",
      "  Pretrain epoch 8/8 - loss 1.0990 - val_acc 61.91%\n",
      "  Epoch 1/20 - train_loss 0.8929 - val_acc 47.80%\n",
      "   -> new best val acc 47.80%\n",
      "  Epoch 1/20 - train_loss 0.8929 - val_acc 47.80%\n",
      "   -> new best val acc 47.80%\n",
      "  Epoch 2/20 - train_loss 0.6970 - val_acc 54.52%\n",
      "   -> new best val acc 54.52%\n",
      "  Epoch 2/20 - train_loss 0.6970 - val_acc 54.52%\n",
      "   -> new best val acc 54.52%\n",
      "  Epoch 3/20 - train_loss 0.6275 - val_acc 59.80%\n",
      "   -> new best val acc 59.80%\n",
      "  Epoch 3/20 - train_loss 0.6275 - val_acc 59.80%\n",
      "   -> new best val acc 59.80%\n",
      "  Epoch 4/20 - train_loss 0.5983 - val_acc 62.52%\n",
      "   -> new best val acc 62.52%\n",
      "  Epoch 4/20 - train_loss 0.5983 - val_acc 62.52%\n",
      "   -> new best val acc 62.52%\n",
      "  Epoch 5/20 - train_loss 0.5811 - val_acc 61.97%\n",
      "  Epoch 5/20 - train_loss 0.5811 - val_acc 61.97%\n",
      "  Epoch 6/20 - train_loss 0.5679 - val_acc 63.23%\n",
      "   -> new best val acc 63.23%\n",
      "  Epoch 6/20 - train_loss 0.5679 - val_acc 63.23%\n",
      "   -> new best val acc 63.23%\n",
      "  Epoch 7/20 - train_loss 0.5604 - val_acc 63.42%\n",
      "   -> new best val acc 63.42%\n",
      "  Epoch 7/20 - train_loss 0.5604 - val_acc 63.42%\n",
      "   -> new best val acc 63.42%\n",
      "  Epoch 8/20 - train_loss 0.5461 - val_acc 63.22%\n",
      "  Epoch 8/20 - train_loss 0.5461 - val_acc 63.22%\n",
      "  Epoch 9/20 - train_loss 0.5378 - val_acc 63.82%\n",
      "   -> new best val acc 63.82%\n",
      "  Epoch 9/20 - train_loss 0.5378 - val_acc 63.82%\n",
      "   -> new best val acc 63.82%\n",
      "  Epoch 10/20 - train_loss 0.5321 - val_acc 63.45%\n",
      "  Epoch 10/20 - train_loss 0.5321 - val_acc 63.45%\n",
      "  Epoch 11/20 - train_loss 0.5240 - val_acc 64.75%\n",
      "   -> new best val acc 64.75%\n",
      "  Epoch 11/20 - train_loss 0.5240 - val_acc 64.75%\n",
      "   -> new best val acc 64.75%\n",
      "  Epoch 12/20 - train_loss 0.5164 - val_acc 65.21%\n",
      "   -> new best val acc 65.21%\n",
      "  Epoch 12/20 - train_loss 0.5164 - val_acc 65.21%\n",
      "   -> new best val acc 65.21%\n",
      "  Epoch 13/20 - train_loss 0.5107 - val_acc 65.04%\n",
      "  Epoch 13/20 - train_loss 0.5107 - val_acc 65.04%\n",
      "  Epoch 14/20 - train_loss 0.5074 - val_acc 65.38%\n",
      "   -> new best val acc 65.38%\n",
      "  Epoch 14/20 - train_loss 0.5074 - val_acc 65.38%\n",
      "   -> new best val acc 65.38%\n",
      "  Epoch 15/20 - train_loss 0.4968 - val_acc 64.54%\n",
      "  Epoch 15/20 - train_loss 0.4968 - val_acc 64.54%\n",
      "  Epoch 16/20 - train_loss 0.4945 - val_acc 64.98%\n",
      "  Epoch 16/20 - train_loss 0.4945 - val_acc 64.98%\n",
      "  Epoch 17/20 - train_loss 0.4887 - val_acc 67.27%\n",
      "   -> new best val acc 67.27%\n",
      "  Epoch 17/20 - train_loss 0.4887 - val_acc 67.27%\n",
      "   -> new best val acc 67.27%\n",
      "  Epoch 18/20 - train_loss 0.4859 - val_acc 66.97%\n",
      "  Epoch 18/20 - train_loss 0.4859 - val_acc 66.97%\n",
      "  Epoch 19/20 - train_loss 0.4791 - val_acc 66.22%\n",
      "  Epoch 19/20 - train_loss 0.4791 - val_acc 66.22%\n",
      "  Epoch 20/20 - train_loss 0.4770 - val_acc 65.31%\n",
      "  Epoch 20/20 - train_loss 0.4770 - val_acc 65.31%\n",
      "Variant v7_3 OOF acc: 66.49%\n",
      "\n",
      "=== Training variant v8_3 (q=8, layers=3) ===\n",
      "\n",
      "Variant v8_3 - Fold 1/3\n",
      " Pretraining encoder (classical)...\n",
      "Variant v7_3 OOF acc: 66.49%\n",
      "\n",
      "=== Training variant v8_3 (q=8, layers=3) ===\n",
      "\n",
      "Variant v8_3 - Fold 1/3\n",
      " Pretraining encoder (classical)...\n",
      "  Pretrain epoch 1/8 - loss 1.3883 - val_acc 27.70%\n",
      "  Pretrain epoch 1/8 - loss 1.3883 - val_acc 27.70%\n",
      "  Pretrain epoch 2/8 - loss 1.3824 - val_acc 28.21%\n",
      "  Pretrain epoch 2/8 - loss 1.3824 - val_acc 28.21%\n",
      "  Pretrain epoch 3/8 - loss 1.3693 - val_acc 48.30%\n",
      "  Pretrain epoch 3/8 - loss 1.3693 - val_acc 48.30%\n",
      "  Pretrain epoch 4/8 - loss 1.3244 - val_acc 48.81%\n",
      "  Pretrain epoch 4/8 - loss 1.3244 - val_acc 48.81%\n",
      "  Pretrain epoch 5/8 - loss 1.2412 - val_acc 56.66%\n",
      "  Pretrain epoch 5/8 - loss 1.2412 - val_acc 56.66%\n",
      "  Pretrain epoch 6/8 - loss 1.1578 - val_acc 56.92%\n",
      "  Pretrain epoch 6/8 - loss 1.1578 - val_acc 56.92%\n",
      "  Pretrain epoch 7/8 - loss 1.0989 - val_acc 57.88%\n",
      "  Pretrain epoch 7/8 - loss 1.0989 - val_acc 57.88%\n",
      "  Pretrain epoch 8/8 - loss 1.0617 - val_acc 62.80%\n",
      "  Pretrain epoch 8/8 - loss 1.0617 - val_acc 62.80%\n",
      "  Epoch 1/20 - train_loss 0.9278 - val_acc 46.52%\n",
      "   -> new best val acc 46.52%\n",
      "  Epoch 1/20 - train_loss 0.9278 - val_acc 46.52%\n",
      "   -> new best val acc 46.52%\n",
      "  Epoch 2/20 - train_loss 0.7180 - val_acc 58.34%\n",
      "   -> new best val acc 58.34%\n",
      "  Epoch 2/20 - train_loss 0.7180 - val_acc 58.34%\n",
      "   -> new best val acc 58.34%\n",
      "  Epoch 3/20 - train_loss 0.6251 - val_acc 59.44%\n",
      "   -> new best val acc 59.44%\n",
      "  Epoch 3/20 - train_loss 0.6251 - val_acc 59.44%\n",
      "   -> new best val acc 59.44%\n",
      "  Epoch 4/20 - train_loss 0.5897 - val_acc 61.48%\n",
      "   -> new best val acc 61.48%\n",
      "  Epoch 4/20 - train_loss 0.5897 - val_acc 61.48%\n",
      "   -> new best val acc 61.48%\n",
      "  Epoch 5/20 - train_loss 0.5718 - val_acc 65.34%\n",
      "   -> new best val acc 65.34%\n",
      "  Epoch 5/20 - train_loss 0.5718 - val_acc 65.34%\n",
      "   -> new best val acc 65.34%\n",
      "  Epoch 6/20 - train_loss 0.5573 - val_acc 64.88%\n",
      "  Epoch 6/20 - train_loss 0.5573 - val_acc 64.88%\n",
      "  Epoch 7/20 - train_loss 0.5447 - val_acc 60.22%\n",
      "  Epoch 7/20 - train_loss 0.5447 - val_acc 60.22%\n",
      "  Epoch 8/20 - train_loss 0.5336 - val_acc 64.64%\n",
      "  Epoch 8/20 - train_loss 0.5336 - val_acc 64.64%\n",
      "  Epoch 9/20 - train_loss 0.5191 - val_acc 64.52%\n",
      "  Epoch 9/20 - train_loss 0.5191 - val_acc 64.52%\n",
      "  Epoch 10/20 - train_loss 0.5107 - val_acc 65.19%\n",
      "  Epoch 10/20 - train_loss 0.5107 - val_acc 65.19%\n",
      "  Epoch 11/20 - train_loss 0.5052 - val_acc 64.80%\n",
      "   -> early stopping\n",
      "  Epoch 11/20 - train_loss 0.5052 - val_acc 64.80%\n",
      "   -> early stopping\n",
      "\n",
      "Variant v8_3 - Fold 2/3\n",
      " Pretraining encoder (classical)...\n",
      "\n",
      "Variant v8_3 - Fold 2/3\n",
      " Pretraining encoder (classical)...\n",
      "  Pretrain epoch 1/8 - loss 1.3894 - val_acc 28.01%\n",
      "  Pretrain epoch 1/8 - loss 1.3894 - val_acc 28.01%\n",
      "  Pretrain epoch 2/8 - loss 1.3821 - val_acc 26.39%\n",
      "  Pretrain epoch 2/8 - loss 1.3821 - val_acc 26.39%\n",
      "  Pretrain epoch 3/8 - loss 1.3670 - val_acc 45.95%\n",
      "  Pretrain epoch 3/8 - loss 1.3670 - val_acc 45.95%\n",
      "  Pretrain epoch 4/8 - loss 1.3206 - val_acc 49.44%\n",
      "  Pretrain epoch 4/8 - loss 1.3206 - val_acc 49.44%\n",
      "  Pretrain epoch 5/8 - loss 1.2344 - val_acc 54.19%\n",
      "  Pretrain epoch 5/8 - loss 1.2344 - val_acc 54.19%\n",
      "  Pretrain epoch 6/8 - loss 1.1533 - val_acc 49.43%\n",
      "  Pretrain epoch 6/8 - loss 1.1533 - val_acc 49.43%\n",
      "  Pretrain epoch 7/8 - loss 1.0949 - val_acc 62.52%\n",
      "  Pretrain epoch 7/8 - loss 1.0949 - val_acc 62.52%\n",
      "  Pretrain epoch 8/8 - loss 1.0541 - val_acc 56.33%\n",
      "  Pretrain epoch 8/8 - loss 1.0541 - val_acc 56.33%\n",
      "  Epoch 1/20 - train_loss 0.9529 - val_acc 43.78%\n",
      "   -> new best val acc 43.78%\n",
      "  Epoch 1/20 - train_loss 0.9529 - val_acc 43.78%\n",
      "   -> new best val acc 43.78%\n",
      "  Epoch 2/20 - train_loss 0.7920 - val_acc 55.61%\n",
      "   -> new best val acc 55.61%\n",
      "  Epoch 2/20 - train_loss 0.7920 - val_acc 55.61%\n",
      "   -> new best val acc 55.61%\n",
      "  Epoch 3/20 - train_loss 0.6675 - val_acc 51.22%\n",
      "  Epoch 3/20 - train_loss 0.6675 - val_acc 51.22%\n",
      "  Epoch 4/20 - train_loss 0.6164 - val_acc 56.56%\n",
      "   -> new best val acc 56.56%\n",
      "  Epoch 4/20 - train_loss 0.6164 - val_acc 56.56%\n",
      "   -> new best val acc 56.56%\n",
      "  Epoch 5/20 - train_loss 0.5851 - val_acc 56.96%\n",
      "   -> new best val acc 56.96%\n",
      "  Epoch 5/20 - train_loss 0.5851 - val_acc 56.96%\n",
      "   -> new best val acc 56.96%\n",
      "  Epoch 6/20 - train_loss 0.5638 - val_acc 62.91%\n",
      "   -> new best val acc 62.91%\n",
      "  Epoch 6/20 - train_loss 0.5638 - val_acc 62.91%\n",
      "   -> new best val acc 62.91%\n",
      "  Epoch 7/20 - train_loss 0.5481 - val_acc 63.82%\n",
      "   -> new best val acc 63.82%\n",
      "  Epoch 7/20 - train_loss 0.5481 - val_acc 63.82%\n",
      "   -> new best val acc 63.82%\n",
      "  Epoch 8/20 - train_loss 0.5349 - val_acc 58.61%\n",
      "  Epoch 8/20 - train_loss 0.5349 - val_acc 58.61%\n",
      "  Epoch 9/20 - train_loss 0.5237 - val_acc 62.99%\n",
      "  Epoch 9/20 - train_loss 0.5237 - val_acc 62.99%\n",
      "  Epoch 10/20 - train_loss 0.5133 - val_acc 64.66%\n",
      "   -> new best val acc 64.66%\n",
      "  Epoch 10/20 - train_loss 0.5133 - val_acc 64.66%\n",
      "   -> new best val acc 64.66%\n",
      "  Epoch 11/20 - train_loss 0.5017 - val_acc 65.18%\n",
      "   -> new best val acc 65.18%\n",
      "  Epoch 11/20 - train_loss 0.5017 - val_acc 65.18%\n",
      "   -> new best val acc 65.18%\n",
      "  Epoch 12/20 - train_loss 0.4948 - val_acc 62.55%\n",
      "  Epoch 12/20 - train_loss 0.4948 - val_acc 62.55%\n",
      "  Epoch 13/20 - train_loss 0.4904 - val_acc 64.57%\n",
      "  Epoch 13/20 - train_loss 0.4904 - val_acc 64.57%\n",
      "  Epoch 14/20 - train_loss 0.4843 - val_acc 62.56%\n",
      "  Epoch 14/20 - train_loss 0.4843 - val_acc 62.56%\n",
      "  Epoch 15/20 - train_loss 0.4800 - val_acc 65.53%\n",
      "   -> new best val acc 65.53%\n",
      "  Epoch 15/20 - train_loss 0.4800 - val_acc 65.53%\n",
      "   -> new best val acc 65.53%\n",
      "  Epoch 16/20 - train_loss 0.4745 - val_acc 63.43%\n",
      "  Epoch 16/20 - train_loss 0.4745 - val_acc 63.43%\n",
      "  Epoch 17/20 - train_loss 0.4712 - val_acc 66.58%\n",
      "   -> new best val acc 66.58%\n",
      "  Epoch 17/20 - train_loss 0.4712 - val_acc 66.58%\n",
      "   -> new best val acc 66.58%\n",
      "  Epoch 18/20 - train_loss 0.4610 - val_acc 66.36%\n",
      "  Epoch 18/20 - train_loss 0.4610 - val_acc 66.36%\n",
      "  Epoch 19/20 - train_loss 0.4594 - val_acc 63.01%\n",
      "  Epoch 19/20 - train_loss 0.4594 - val_acc 63.01%\n",
      "  Epoch 20/20 - train_loss 0.4599 - val_acc 64.32%\n",
      "  Epoch 20/20 - train_loss 0.4599 - val_acc 64.32%\n",
      "\n",
      "Variant v8_3 - Fold 3/3\n",
      " Pretraining encoder (classical)...\n",
      "\n",
      "Variant v8_3 - Fold 3/3\n",
      " Pretraining encoder (classical)...\n",
      "  Pretrain epoch 1/8 - loss 1.3876 - val_acc 26.17%\n",
      "  Pretrain epoch 1/8 - loss 1.3876 - val_acc 26.17%\n",
      "  Pretrain epoch 2/8 - loss 1.3740 - val_acc 32.63%\n",
      "  Pretrain epoch 2/8 - loss 1.3740 - val_acc 32.63%\n",
      "  Pretrain epoch 3/8 - loss 1.3384 - val_acc 41.93%\n",
      "  Pretrain epoch 3/8 - loss 1.3384 - val_acc 41.93%\n",
      "  Pretrain epoch 4/8 - loss 1.2794 - val_acc 51.01%\n",
      "  Pretrain epoch 4/8 - loss 1.2794 - val_acc 51.01%\n",
      "  Pretrain epoch 5/8 - loss 1.2051 - val_acc 56.27%\n",
      "  Pretrain epoch 5/8 - loss 1.2051 - val_acc 56.27%\n",
      "  Pretrain epoch 6/8 - loss 1.1450 - val_acc 61.24%\n",
      "  Pretrain epoch 6/8 - loss 1.1450 - val_acc 61.24%\n",
      "  Pretrain epoch 7/8 - loss 1.0876 - val_acc 62.21%\n",
      "  Pretrain epoch 7/8 - loss 1.0876 - val_acc 62.21%\n",
      "  Pretrain epoch 8/8 - loss 1.0525 - val_acc 63.75%\n",
      "  Pretrain epoch 8/8 - loss 1.0525 - val_acc 63.75%\n",
      "  Epoch 1/20 - train_loss 0.9161 - val_acc 48.74%\n",
      "   -> new best val acc 48.74%\n",
      "  Epoch 1/20 - train_loss 0.9161 - val_acc 48.74%\n",
      "   -> new best val acc 48.74%\n",
      "  Epoch 2/20 - train_loss 0.7056 - val_acc 52.52%\n",
      "   -> new best val acc 52.52%\n",
      "  Epoch 2/20 - train_loss 0.7056 - val_acc 52.52%\n",
      "   -> new best val acc 52.52%\n",
      "  Epoch 3/20 - train_loss 0.5967 - val_acc 59.66%\n",
      "   -> new best val acc 59.66%\n",
      "  Epoch 3/20 - train_loss 0.5967 - val_acc 59.66%\n",
      "   -> new best val acc 59.66%\n",
      "  Epoch 4/20 - train_loss 0.5635 - val_acc 61.11%\n",
      "   -> new best val acc 61.11%\n",
      "  Epoch 4/20 - train_loss 0.5635 - val_acc 61.11%\n",
      "   -> new best val acc 61.11%\n",
      "  Epoch 5/20 - train_loss 0.5442 - val_acc 65.94%\n",
      "   -> new best val acc 65.94%\n",
      "  Epoch 5/20 - train_loss 0.5442 - val_acc 65.94%\n",
      "   -> new best val acc 65.94%\n",
      "  Epoch 6/20 - train_loss 0.5296 - val_acc 57.36%\n",
      "  Epoch 6/20 - train_loss 0.5296 - val_acc 57.36%\n",
      "  Epoch 7/20 - train_loss 0.5169 - val_acc 57.70%\n",
      "  Epoch 7/20 - train_loss 0.5169 - val_acc 57.70%\n",
      "  Epoch 8/20 - train_loss 0.5067 - val_acc 63.10%\n",
      "  Epoch 8/20 - train_loss 0.5067 - val_acc 63.10%\n",
      "  Epoch 9/20 - train_loss 0.5009 - val_acc 60.56%\n",
      "  Epoch 9/20 - train_loss 0.5009 - val_acc 60.56%\n",
      "  Epoch 10/20 - train_loss 0.4928 - val_acc 62.25%\n",
      "  Epoch 10/20 - train_loss 0.4928 - val_acc 62.25%\n",
      "  Epoch 11/20 - train_loss 0.4846 - val_acc 62.91%\n",
      "   -> early stopping\n",
      "  Epoch 11/20 - train_loss 0.4846 - val_acc 62.91%\n",
      "   -> early stopping\n",
      "Variant v8_3 OOF acc: 65.95%\n",
      "\n",
      "Soft-average ensemble OOF accuracy: 66.44%\n",
      "Soft-average ensemble classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7740    0.2200    0.3426     12000\n",
      "           1     0.5874    0.8659    0.7000     12000\n",
      "           2     0.9986    0.9398    0.9683     12000\n",
      "           3     0.4859    0.6318    0.5493     12000\n",
      "\n",
      "    accuracy                         0.6644     48000\n",
      "   macro avg     0.7115    0.6644    0.6401     48000\n",
      "weighted avg     0.7115    0.6644    0.6401     48000\n",
      "\n",
      "Variant v8_3 OOF acc: 65.95%\n",
      "\n",
      "Soft-average ensemble OOF accuracy: 66.44%\n",
      "Soft-average ensemble classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7740    0.2200    0.3426     12000\n",
      "           1     0.5874    0.8659    0.7000     12000\n",
      "           2     0.9986    0.9398    0.9683     12000\n",
      "           3     0.4859    0.6318    0.5493     12000\n",
      "\n",
      "    accuracy                         0.6644     48000\n",
      "   macro avg     0.7115    0.6644    0.6401     48000\n",
      "weighted avg     0.7115    0.6644    0.6401     48000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\QUANTUM COMPUTING\\Industry Projects\\.venv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1272: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.8. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stacked (logistic) OOF accuracy: 67.76%\n",
      "Stacked classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5967    0.4274    0.4981     12000\n",
      "           1     0.6039    0.9182    0.7286     12000\n",
      "           2     0.9770    0.9823    0.9797     12000\n",
      "           3     0.5048    0.3826    0.4353     12000\n",
      "\n",
      "    accuracy                         0.6776     48000\n",
      "   macro avg     0.6706    0.6776    0.6604     48000\n",
      "weighted avg     0.6706    0.6776    0.6604     48000\n",
      "\n",
      "\n",
      "Confusion matrix (soft-average):\n",
      "        Pred_0  Pred_1  Pred_2  Pred_3\n",
      "True_0    2640    2932       8    6420\n",
      "True_1       0   10391       6    1603\n",
      "True_2       0     722   11278       0\n",
      "True_3     771    3645       2    7582\n",
      "\n",
      "Confusion matrix (stacked):\n",
      "        Pred_0  Pred_1  Pred_2  Pred_3\n",
      "True_0    5129    3126      67    3678\n",
      "True_1       0   11018     157     825\n",
      "True_2       0     212   11788       0\n",
      "True_3    3466    3890      53    4591\n",
      "\n",
      "All done. Models saved. Review OOF metrics and pick your final submission (soft-average or stacked).\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "import time\n",
    "import copy\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "import pennylane as qml\n",
    "\n",
    "# -------------------------\n",
    "# CONFIGURATION (tweak for runtime)\n",
    "# -------------------------\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "DATA_FILES = [\n",
    "    \"C0_dataset.csv\",\n",
    "    \"C1_dataset.csv\",\n",
    "    \"C2_dataset.csv\",\n",
    "    \"C3_dataset.csv\",\n",
    "]\n",
    "FEATURES = [\"lambda\", \"beta\", \"frac_campo\"]\n",
    "LABEL = \"ion\"\n",
    "\n",
    "# Variants to train (qubits, layers, encoder_hidden)\n",
    "VARIANTS = [\n",
    "    {\"name\": \"v6_2\", \"n_qubits\": 6, \"n_layers\": 2, \"enc_hidden\": 48},\n",
    "    {\"name\": \"v7_3\", \"n_qubits\": 7, \"n_layers\": 3, \"enc_hidden\": 48},\n",
    "    {\"name\": \"v8_3\", \"n_qubits\": 8, \"n_layers\": 3, \"enc_hidden\": 64},\n",
    "]\n",
    "\n",
    "KFOLD = 3\n",
    "BATCH_SIZE = 128\n",
    "LR_PRETRAIN = 1e-3\n",
    "LR_HYBRID = 5e-4\n",
    "PRETRAIN_EPOCHS = 8     # pretrain encoder classically\n",
    "HYBRID_EPOCHS = 20      # hybrid training\n",
    "PATIENCE = 6\n",
    "DEVICE = torch.device(\"cpu\")  # QML sim runs on CPU\n",
    "\n",
    "# Loss tuning (gentle upweight for class-3)\n",
    "MANUAL_CLASS_WEIGHTS = np.array([1.0, 1.0, 1.0, 1.2], dtype=np.float32)\n",
    "FOCAL_GAMMA = 1.5\n",
    "\n",
    "# -------------------------\n",
    "# Helpers\n",
    "# -------------------------\n",
    "def load_and_prepare():\n",
    "    df_list = [pd.read_csv(p) for p in DATA_FILES]\n",
    "    df = pd.concat(df_list, ignore_index=True)\n",
    "    X = df[FEATURES].values.astype(np.float32)\n",
    "    y = df[LABEL].values.astype(np.int64)\n",
    "    return X, y, df\n",
    "\n",
    "def make_dataloader(X, y, batch_size=BATCH_SIZE, shuffle=True):\n",
    "    X_t = torch.from_numpy(X).float()\n",
    "    y_t = torch.from_numpy(y).long()\n",
    "    ds = TensorDataset(X_t, y_t)\n",
    "    return DataLoader(ds, batch_size=batch_size, shuffle=shuffle, drop_last=False)\n",
    "\n",
    "# Focal loss implementation\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, gamma=2.0, weight=None, reduction='mean'):\n",
    "        super().__init__()\n",
    "        self.gamma = gamma\n",
    "        if weight is not None:\n",
    "            self.register_buffer('weight', torch.tensor(weight, dtype=torch.float32))\n",
    "        else:\n",
    "            self.weight = None\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        ce = F.cross_entropy(inputs, targets, weight=self.weight, reduction='none')\n",
    "        pt = torch.exp(-ce)\n",
    "        loss = ((1 - pt) ** self.gamma) * ce\n",
    "        if self.reduction == 'mean':\n",
    "            return loss.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            return loss.sum()\n",
    "        else:\n",
    "            return loss\n",
    "\n",
    "# -------------------------\n",
    "# Model factories (QNode + TorchLayer) per variant\n",
    "# -------------------------\n",
    "def create_qnode_and_torchlayer(n_qubits, n_layers):\n",
    "    dev = qml.device(\"default.qubit\", wires=n_qubits)\n",
    "\n",
    "    def circuit(inputs, weights):\n",
    "        qml.AngleEmbedding(inputs, wires=range(n_qubits))\n",
    "        qml.StronglyEntanglingLayers(weights, wires=range(n_qubits))\n",
    "        return [qml.expval(qml.PauliZ(i)) for i in range(n_qubits)]\n",
    "\n",
    "    qnode = qml.qnode(dev, interface=\"torch\")(circuit)\n",
    "    weight_shapes = {\"weights\": (n_layers, n_qubits, 3)}\n",
    "    return qnode, weight_shapes\n",
    "\n",
    "# Encoder and Hybrid model definitions\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, in_dim=3, hidden=48, latent_dim=6):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(in_dim, hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.12),\n",
    "            nn.Linear(hidden, latent_dim)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class HybridQNN(nn.Module):\n",
    "    def __init__(self, qnode, weight_shapes, n_qubits, n_layers, enc_hidden):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(in_dim=len(FEATURES), hidden=enc_hidden, latent_dim=n_qubits)\n",
    "        self.q_layer = qml.qnn.TorchLayer(qnode, weight_shapes)\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(n_qubits, max(24, n_qubits*3)),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.12),\n",
    "            nn.Linear(max(24, n_qubits*3), 4)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        latent = self.encoder(x)\n",
    "        angles = torch.tanh(latent) * math.pi\n",
    "        q_out = self.q_layer(angles)\n",
    "        logits = self.head(q_out)\n",
    "        return logits\n",
    "\n",
    "# -------------------------\n",
    "# Training helpers\n",
    "# -------------------------\n",
    "def pretrain_encoder(encoder, X_train, y_train, X_val, y_val, hidden_lr=LR_PRETRAIN, epochs=PRETRAIN_EPOCHS):\n",
    "    # Train encoder + small dense head (classical) to give encoder a good initialization\n",
    "    latent_dim = encoder.net[-1].out_features\n",
    "    head = nn.Linear(latent_dim, 4)\n",
    "    model = nn.Sequential(encoder, head).to(DEVICE)\n",
    "    opt = optim.Adam(model.parameters(), lr=hidden_lr)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    train_loader = make_dataloader(X_train, y_train, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    val_loader = make_dataloader(X_val, y_val, batch_size=BATCH_SIZE, shuffle=False)\n",
    "    best_w = None\n",
    "    best_acc = 0.0\n",
    "    no_improve = 0\n",
    "    for epoch in range(1, epochs+1):\n",
    "        model.train()\n",
    "        run_loss = 0.0\n",
    "        for xb, yb in train_loader:\n",
    "            opt.zero_grad()\n",
    "            out = model(xb.to(DEVICE))\n",
    "            loss = loss_fn(out, yb.to(DEVICE))\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            run_loss += loss.item() * xb.size(0)\n",
    "        run_loss /= len(train_loader.dataset)\n",
    "        # val\n",
    "        model.eval()\n",
    "        preds = []\n",
    "        labs = []\n",
    "        with torch.no_grad():\n",
    "            for xb, yb in val_loader:\n",
    "                out = model(xb.to(DEVICE))\n",
    "                p = torch.softmax(out, dim=1)\n",
    "                _, pred = torch.max(p, dim=1)\n",
    "                preds.extend(pred.cpu().numpy())\n",
    "                labs.extend(yb.numpy())\n",
    "        acc = accuracy_score(labs, preds)\n",
    "        print(f\"  Pretrain epoch {epoch}/{epochs} - loss {run_loss:.4f} - val_acc {acc*100:.2f}%\")\n",
    "        if acc > best_acc + 1e-8:\n",
    "            best_acc = acc\n",
    "            best_w = copy.deepcopy(model.state_dict())\n",
    "            no_improve = 0\n",
    "        else:\n",
    "            no_improve += 1\n",
    "            if no_improve >= 5:\n",
    "                break\n",
    "    if best_w is not None:\n",
    "        # load best weights back into the model (this updates the encoder in-place)\n",
    "        model.load_state_dict(best_w)\n",
    "    # encoder is the same object used inside model, so its weights are already updated; just return it\n",
    "    return encoder\n",
    "\n",
    "def train_hybrid_kfold(variant, X, y, focal_gamma=FOCAL_GAMMA, class_weights=MANUAL_CLASS_WEIGHTS):\n",
    "    \"\"\"\n",
    "    Train a single variant with KFOLD and return OOF probs and saved model files+scalers per fold.\n",
    "    \"\"\"\n",
    "    name = variant[\"name\"]\n",
    "    n_qubits = variant[\"n_qubits\"]\n",
    "    n_layers = variant[\"n_layers\"]\n",
    "    enc_hidden = variant[\"enc_hidden\"]\n",
    "\n",
    "    qnode, weight_shapes = create_qnode_and_torchlayer(n_qubits, n_layers)\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=KFOLD, shuffle=True, random_state=SEED)\n",
    "    oof_probs = np.zeros((len(y), 4), dtype=np.float32)\n",
    "    fold_models = []  # list of tuples (model_file, scaler)\n",
    "\n",
    "    fold = 0\n",
    "    for train_idx, val_idx in skf.split(X, y):\n",
    "        fold += 1\n",
    "        print(f\"\\nVariant {name} - Fold {fold}/{KFOLD}\")\n",
    "        X_train, y_train = X[train_idx], y[train_idx]\n",
    "        X_val, y_val = X[val_idx], y[val_idx]\n",
    "\n",
    "        scaler = StandardScaler()\n",
    "        X_train_s = scaler.fit_transform(X_train)\n",
    "        X_val_s = scaler.transform(X_val)\n",
    "\n",
    "        # Pretrain encoder on this fold (classical)\n",
    "        encoder = Encoder(in_dim=len(FEATURES), hidden=enc_hidden, latent_dim=n_qubits)\n",
    "        print(\" Pretraining encoder (classical)...\")\n",
    "        encoder = pretrain_encoder(encoder, X_train_s, y_train, X_val_s, y_val, epochs=PRETRAIN_EPOCHS)\n",
    "\n",
    "        # Build hybrid model and copy encoder weights\n",
    "        model = HybridQNN(qnode, weight_shapes, n_qubits=n_qubits, n_layers=n_layers, enc_hidden=enc_hidden).to(DEVICE)\n",
    "        # copy encoder weights from pretrain encoder (direct load of state_dict)\n",
    "        model.encoder.load_state_dict(encoder.state_dict())\n",
    "\n",
    "        loss_fn = FocalLoss(gamma=focal_gamma, weight=class_weights, reduction='mean')\n",
    "        optimizer = optim.Adam(model.parameters(), lr=LR_HYBRID)\n",
    "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=\"max\", factor=0.5, patience=3)\n",
    "\n",
    "        best_val_acc = 0.0\n",
    "        best_w = None\n",
    "        no_improve = 0\n",
    "\n",
    "        train_loader = make_dataloader(X_train_s, y_train, batch_size=BATCH_SIZE, shuffle=True)\n",
    "        val_loader = make_dataloader(X_val_s, y_val, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "        for epoch in range(1, HYBRID_EPOCHS + 1):\n",
    "            model.train()\n",
    "            run_loss = 0.0\n",
    "            for xb, yb in train_loader:\n",
    "                optimizer.zero_grad()\n",
    "                logits = model(xb.to(DEVICE))\n",
    "                loss = loss_fn(logits, yb.to(DEVICE))\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                run_loss += loss.item() * xb.size(0)\n",
    "            run_loss /= len(train_loader.dataset)\n",
    "\n",
    "            # eval\n",
    "            model.eval()\n",
    "            preds = []\n",
    "            probs = []\n",
    "            labs = []\n",
    "            with torch.no_grad():\n",
    "                for xb, yb in val_loader:\n",
    "                    out = model(xb.to(DEVICE))\n",
    "                    p = torch.softmax(out, dim=1)\n",
    "                    _, pred = torch.max(p, dim=1)\n",
    "                    preds.extend(pred.cpu().numpy())\n",
    "                    probs.extend(p.cpu().numpy())\n",
    "                    labs.extend(yb.numpy())\n",
    "            val_acc = accuracy_score(labs, preds)\n",
    "            scheduler.step(val_acc)\n",
    "            print(f\"  Epoch {epoch}/{HYBRID_EPOCHS} - train_loss {run_loss:.4f} - val_acc {val_acc*100:.2f}%\")\n",
    "            if val_acc > best_val_acc + 1e-8:\n",
    "                best_val_acc = val_acc\n",
    "                best_w = copy.deepcopy(model.state_dict())\n",
    "                no_improve = 0\n",
    "                print(f\"   -> new best val acc {best_val_acc*100:.2f}%\")\n",
    "            else:\n",
    "                no_improve += 1\n",
    "                if no_improve >= PATIENCE:\n",
    "                    print(\"   -> early stopping\")\n",
    "                    break\n",
    "\n",
    "        # load best, save model and scaler\n",
    "        model.load_state_dict(best_w)\n",
    "        model_file = f\"{name}_fold{fold}.pth\"\n",
    "        torch.save(model.state_dict(), model_file)\n",
    "        fold_models.append((model_file, scaler))\n",
    "\n",
    "        # produce OOF probs for this fold\n",
    "        model.eval()\n",
    "        probs = []\n",
    "        with torch.no_grad():\n",
    "            for xb, yb in val_loader:\n",
    "                out = model(xb.to(DEVICE))\n",
    "                p = torch.softmax(out, dim=1)\n",
    "                probs.append(p.cpu().numpy())\n",
    "        probs = np.vstack(probs)\n",
    "        oof_probs[val_idx] = probs\n",
    "\n",
    "    return {\"name\": name, \"variant\": variant, \"oof_probs\": oof_probs, \"fold_models\": fold_models}\n",
    "\n",
    "# -------------------------\n",
    "# Ensembling / stacking helpers\n",
    "# -------------------------\n",
    "def ensemble_average(probs_list):\n",
    "    # probs_list: list of (n_samples, n_classes)\n",
    "    stacked = np.mean(np.stack(probs_list, axis=0), axis=0)\n",
    "    preds = np.argmax(stacked, axis=1)\n",
    "    return preds, stacked\n",
    "\n",
    "def train_meta_and_predict(oof_prob_matrix, y):\n",
    "    # oof_prob_matrix: (n_samples, num_models * n_classes)\n",
    "    # train logistic regression on OOF to predict labels\n",
    "    meta = LogisticRegression(max_iter=2000, multi_class='multinomial', solver='lbfgs')\n",
    "    meta.fit(oof_prob_matrix, y)\n",
    "    preds = meta.predict(oof_prob_matrix)\n",
    "    probs = meta.predict_proba(oof_prob_matrix)\n",
    "    return meta, preds, probs\n",
    "\n",
    "# -------------------------\n",
    "# MAIN PIPELINE\n",
    "# -------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    X, y, df = load_and_prepare()\n",
    "    n_samples = len(y)\n",
    "    print(f\"Loaded {n_samples} samples, class counts: {np.bincount(y)}\")\n",
    "\n",
    "    variant_results = []\n",
    "    # Train each variant and collect OOF probs\n",
    "    for var in VARIANTS:\n",
    "        print(f\"\\n=== Training variant {var['name']} (q={var['n_qubits']}, layers={var['n_layers']}) ===\")\n",
    "        res = train_hybrid_kfold(var, X, y, focal_gamma=FOCAL_GAMMA, class_weights=MANUAL_CLASS_WEIGHTS)\n",
    "        variant_results.append(res)\n",
    "        # quick sanity\n",
    "        oof_preds = np.argmax(res[\"oof_probs\"], axis=1)\n",
    "        print(f\"Variant {var['name']} OOF acc: {accuracy_score(y, oof_preds)*100:.2f}%\")\n",
    "\n",
    "    # Build OOF matrix for stacking\n",
    "    probs_list = [res[\"oof_probs\"] for res in variant_results]\n",
    "    # simple average ensemble\n",
    "    avg_preds, avg_probs = ensemble_average(probs_list)\n",
    "    avg_acc = accuracy_score(y, avg_preds)\n",
    "    print(f\"\\nSoft-average ensemble OOF accuracy: {avg_acc*100:.2f}%\")\n",
    "    print(\"Soft-average ensemble classification report:\")\n",
    "    print(classification_report(y, avg_preds, digits=4))\n",
    "\n",
    "    # Stacking: concatenate OOF probs from each variant as features\n",
    "    oof_feature_matrix = np.concatenate(probs_list, axis=1)  # shape (n_samples, num_models * n_classes)\n",
    "    meta_model, meta_preds, meta_probs = train_meta_and_predict(oof_feature_matrix, y)\n",
    "    meta_acc = accuracy_score(y, meta_preds)\n",
    "    print(f\"\\nStacked (logistic) OOF accuracy: {meta_acc*100:.2f}%\")\n",
    "    print(\"Stacked classification report:\")\n",
    "    print(classification_report(y, meta_preds, digits=4))\n",
    "\n",
    "    # Save artifacts\n",
    "    np.save(\"oof_probs_variants.npy\", np.stack(probs_list, axis=0))  # (num_variants, n_samples, n_classes)\n",
    "    np.save(\"ensemble_avg_probs.npy\", avg_probs)\n",
    "    np.save(\"stacked_oof_probs.npy\", meta_probs)\n",
    "    # Save meta model (pickle)\n",
    "    import joblib\n",
    "    joblib.dump(meta_model, \"meta_logistic.pkl\")\n",
    "\n",
    "    # Show confusion matrices\n",
    "    print(\"\\nConfusion matrix (soft-average):\")\n",
    "    print(pd.DataFrame(confusion_matrix(y, avg_preds), index=[f\"True_{i}\" for i in range(4)], columns=[f\"Pred_{i}\" for i in range(4)]))\n",
    "    print(\"\\nConfusion matrix (stacked):\")\n",
    "    print(pd.DataFrame(confusion_matrix(y, meta_preds), index=[f\"True_{i}\" for i in range(4)], columns=[f\"Pred_{i}\" for i in range(4)]))\n",
    "\n",
    "    print(\"\\nAll done. Models saved. Review OOF metrics and pick your final submission (soft-average or stacked).\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
